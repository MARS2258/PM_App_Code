import math
import numpy as np
def sigmoid_activate (input):
    d1, d2= np.shape(input)
    out= np.empty(shape= input.shape)
    for i in range(d1):
        for j in range(d2):
            x= input[i][j]
            y=1/(1+math.e**(-x))
            out[i][j]= y
    return out
#a1= np.random.rand(4,5)
#print(a1)
#a2= sigmoid_activate(a1)
#print(a2)

import math
def Loss (input, class_targets):
    #to prevent the zero from affecting in the log()
    d1,d2= np.shape(input)
    loss=0
    #to prevent the zero from affecting in the log()
    modified_input= np.clip(input, a_max= 1-1e-7, a_min= 1e-7)
    #print(modified_input)
    predicted= modified_input[range(len(input)), class_targets] #this acts like the zip function and makes the tuple for representing the elements of the 2d matrix
    #print(np.log(predicted))
    loss=-(class_targets*np.log(predicted))
    return loss, np.mean(loss)
   
#arr=[[0,0.3,0.5],[0.4, 0.3, 0.3],[0.8, 0.15, 0.05]]
#X = np.array(arr)
#cls_trg= [0,1,1]
#mean_loss, loss= Loss(X, cls_trg)
#print(mean_loss)
#print(loss)

import numpy as np
X= [1.1,1.6,1.2]

class Layer_Dense:
    def __init__(self, n_inputs, n_outputs):
        self.weights= np.random.rand(n_inputs, n_outputs)
        self.biases = np.zeros(shape=(1, n_outputs))
    def forward(self, inputs):
        self.output= np.dot(inputs, self.weights)+ self.biases
        
import nnfs
import matplotlib
import numpy as np

X=[1.3 , 1.4, 1.6]
Layer_1= Layer_Dense(3,5)
Layer_2= Layer_Dense(5,2)
Layer_1.forward(X)
Activation_1= sigmoid_activate(Layer_1.output)
Layer_2.forward(Activation_1)
Activation_2 = sigmoid_activate(Layer_2.output)
Output = Activation_2.copy()
Loss_func, avg_loss= Loss(Output, (0,1))

lowest_loss= 999999
best_layer1_weights = Layer_1.weights
best_layer1_biases = Layer_1.biases
best_layer2_weights = Layer_2.weights
best_layer2_biases = Layer_2.biases

for iteration in range( 100000):
    Layer_1.weights+= np.random.rand(3,5)
    Layer_1.biases+= np.random.rand(1,5)
    Layer_2.weights+= np.random.rand(5,2)
    Layer_2.biases+= np.random.rand(1,2)
    
    Layer_1.forward(X)
    Activation_1= sigmoid_activate(Layer_1.output)
    Layer_2.forward(Activation_1)
    Activation_2 = sigmoid_activate(Layer_2.output)
    Output = Activation_2.copy()
    Loss_func, avg_loss= Loss(Output, (0,1))
    
    if avg_loss<lowest_loss:
        best_layer1_weights = Layer_1.weights
        best_layer1_biases = Layer_1.biases
        best_layer2_weights = Layer_2.weights
        best_layer2_biases = Layer_2.biases
        lowest_loss= avg_loss
    
    else :
        Layer_1.weights= best_layer1_weights.copy()
        Layer_1.biases= best_layer1_biases.copy()
        Layer_2.weights= best_layer2_weights.copy()
        Layer_2.biases= best_layer2_biases.copy()
    
print(lowest_loss)
print(Output)
